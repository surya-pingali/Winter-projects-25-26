# -*- coding: utf-8 -*-
"""Assignment1_ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_OIuxdnO-FT08gkEFe3NflKjRv2oVTOO

**SUBMISSION INSTRUCTIONS**

It is recommendend that you make a copy of this colab file and then solve the assignment and upload your final notebook on github.

Before uploading your downloaded notebook, **RENAME** the file as **rollno_name.ipynb**

**Submission Deadline : 9/12/2025 Tuesday EOD i.e before 11:59 PM**

The deadline is strict and will not be extended, Late submissions are not allowed

Note that you have to upload your solution on the github page of the project Vision Transformer and under Week0

**Github Submission repo** -
https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Week0

#**Assignment 1**
#**Section 1 (Python)**
#**Problem 1**
In this problem, you will implement a class that represents a data sample with numerical features.  
##  Problem Statement
Create a class named **`DataSample`** that stores:

- A list of numeric **features**
- A string **label**

You must implement the following **three methods** with **exact names** (do NOT rename them):

| Method | Description |
|--------|------------|
| `__init__(self, features, label)` | Initializes object attributes |
| `min_max_norm(self)` | Apply min-max normalization **in-place** |
| `scaled(self, factor)` | Return a **new list** with each feature multiplied by `factor` |

### Min–Max Normalization Formula

![Alt text for the image](https://miro.medium.com/v2/resize:fit:964/1*OnCBKS-Thqa43qNslohDpA.png)


###  Edge Case
If **all features are equal**, then max = min → division by zero.  
Handle this condition by setting all normalized values to **0**.

---

### Your output must behave conceptually like this (not real execution here):
```python
sample = DataSample([10, 20, 30], "cat")
sample.min_max_norm()
print(sample.features)   # expected -> [0.0, 0.5, 1.0]

print(sample.scaled(2))  # expected -> [0.0, 1.0, 2.0]

**Sample Class**
"""

class DataSample:
  def __init__(self,features,label):
    self.features=features
    self.label=label

  def min_max_norm(self):
    f_min=min(self.features)
    f_max=max(self.features)
    if f_max==f_min:
      self.features=[0.0 for _ in self.features]
    else:
      self.features=[(x-f_min)/(f_max-f_min) for x in self.features]

  def scaled(self,factor):
    return [x*factor for x in self.features]

"""You can check if your code is working correctly using the sample case below"""

sample = DataSample([3.5, -2.0, 3.5, 10.0, 0.0], "bird")

print("Original features:", sample.features)
print("Label:", sample.label)

sample.min_max_norm()
print("After min-max normalization:", sample.features)

scaled_output = sample.scaled(4.2)
print("After scaling x 4.2 (new list expected):", scaled_output)

# Expected Output:
# Original features: [3.5, -2.0, 3.5, 10.0, 0.0]
# Label: bird
# After min-max normalization: [0.4583, 0.0, 0.4583, 1.0, 0.1667]  (approx values)
# After scaling x 4.2: [1.92486, 0.0, 1.92486, 4.2, 0.70014]

"""#**Problem 2**
Sort Based on Unique Character Count

Write a function named **`sort_by_unique_chars`** that sorts a list of strings based on the number of **unique characters** in each string (**descending** order).  
If two strings have the same number of unique characters, sort them **alphabetically**.

Assume that the characters in the words are only consisting of lower-case english alphabets

### Function Definition (DO NOT CHANGE THIS NAME)

```python
def sort_by_unique_chars(words):
  n=len(words)
  for i in range(0,n):
    for j in range(0,n-i-1):
      count1=count_unique_char(wrods[j])
      count2=count_unique_chars(words[j+1])
      if count1<count2:
        words[j], words[j+1]=words[j+1],words[j]
      elif count1==count2 and words[j]>words[j+1]:
        words[j], words[j+1]=words[j+1],words[j]

    return words



    pass

check your code by running the below block
"""

def count_unique_char(word):
  unique_chars=[]
  for char in word:
    seen=False
    for u in unique_chars:
      if u==char:
        seen=True
        break
    if not seen:
      unique_chars.append(char)
  return len(unique_chars)

def sort_by_unique_chars(words):
  n=len(words)
  for i in range(0,n):
    for j in range(0,n-i-1):
      count1=count_unique_char(words[j])
      count2=count_unique_char(words[j+1])
      if count1<count2:
        words[j], words[j+1]=words[j+1],words[j]
      elif count1==count2 and words[j]>words[j+1]:
        words[j], words[j+1]=words[j+1],words[j]

  return words

#example case
input_data = ["apple", "banana", "kiwi", "grape", "mango"]
output = sort_by_unique_chars(input_data)
print(output)
# Expected Output:
# ['grape', 'mango', 'apple', 'banana', 'kiwi']

"""# **Section 2 (NumPy)**
# **Problem 1**

In this problem, you will work with NumPy arrays to practice masking, slicing, advanced indexing, and broadcasting. Follow each step sequentially using only NumPy operations (no Python loops).

## **Problem Statement**

1. **Generate** a 10×10 NumPy array `X` containing random integers between **0 and 100** (inclusive).

2. Create a **boolean mask** selecting all values between **20 and 50** (inclusive).   
   Using this mask, replace those values in `X` with **−1** *in-place*.

3. Extract a **6×6 submatrix** `sub` from the modified `X`:
   - Rows **2 to 8** ( 8 excluded )
   - Columns **3 to 9** ( 9 excluded )

4. Using **advanced NumPy indexing**, extract all **diagonal elements** of the 6×6 submatrix `sub` into a 1D array `diag_vals`.  
   *Hint: use* `np.arange(6)`.

5. Construct a **10×10 structured matrix** `M` using broadcasting, where:

    `M[i, j] = (i - j)²`

    for all `0 ≤ i, j < 10`. This must be done **without loops**.

"""

import numpy as np
x=np.random.randint(0,101,(10,10))
mask=(x>=20)&(x<=50)
x[mask]=-1
sub=x[2:8,3:9]
idx=np.arange(6)
diag_vals=sub[idx,idx]
i=np.arange(10)[:,None]
j=np.arange(10)[None,:]
m=i-j
m=m**2
print("X (after replacement):\n", x, "\n")
print("6x6 submatrix 'sub':\n", sub, "\n")
print("Diagonal of sub (diag_vals):\n", diag_vals, "\n")
print("Structured matrix M:\n", m)

"""# **Problem 2**

In this problem, you will work with NumPy arrays representing class scores for multiple samples.
You will practice row-wise normalization, broadcasting, and boolean masking.

## **Problem Statement**

1. Create a NumPy array `scores` of shape 5×4 containing integer values between **0 and 20** (inclusive).  
   Each row represents a sample, and each column represents a score for one of the 4 classes.

2. For each row in `scores`, subtract the **maximum value of that row** from all elements in that row.  
   This operation must be performed using **broadcasting** (no loops allowed).

   *Hint:*  
   `scores.max(axis=1, keepdims=True)` produces a (5×1) column of row-wise maxima.

3. Compute a new array `exp_scores` by applying the exponential function to each element:

   `exp_scores = np.exp(shifted_scores)`

4. **Normalize each row** of `exp_scores` so that each row sums to **1**.  
   Store the resulting array in `probs`.  

   The transformation is conceptually shown by the formula:

   <img src="https://miro.medium.com/v2/resize:fit:300/1*bol3L-WNVacCscvG-rlypQ.png" width="250"/>

   Which corresponds to:

   `probs[i, j] = exp_scores[i, j]/sum(exp_scores[i, :])`

5. Compute the **predicted class** for each sample by taking the index of the largest value in each row of `probs`.  
   Store this in a 1D array `y_pred` of length 5.

6. Create a NumPy array `y_true` of length 5 containing the true class labels (each between 0 and 3).

7. Create a boolean array `correct_mask` indicating whether each predicted label matches the true label.  
   Then compute the **accuracy** using:

   `accuracy = correct_mask.mean()`

"""

import numpy as np
scores=np.random.randint(0,21,(5,4))
row_max=scores.max(axis=1,keepdims=True)
shifted_scores=scores-row_max
exp_scores=np.exp(shifted_scores)
row_sum=exp_scores.sum(axis=1,keepdims=True)
probs=exp_scores/row_sum
y_pred=np.argmax(probs,axis=1)
y_true=np.random.randint(0,4,5)
correct_mask=(y_pred==y_true)
accuracy=correct_mask.mean()

"""#Section 3 - Pandas and MatPlotLib

#Creating Batches for Training and Testing Using Pandas

Download the following .csv file from the given link directly using commands in colab - url = "https://raw.githubusercontent.com/rashida048/Datasets/master/StudentsPerformance.csv"
"""

!wget "https://raw.githubusercontent.com/rashida048/Datasets/master/StudentsPerformance.csv"

import pandas as pd

"""Import the csv file as a dataframe"""

df = pd.read_csv("StudentsPerformance.csv")

"""Write the command to be able to see the first 5 rows"""

df.head()

"""Create multiple dataframes with the following columns sorted : gender, race, math, reading and writing scores.

(meaning create df1, df2, df3 etc. with the math score sorted in df1, gender wise sorted in df2 etc.)
"""

# gender

# race/ethnicity

# math

# reading

# writing

"""Create 2 non-overlapping dataframes test_df and train_df such that 20% of the rows are in test_df and the rest in train_df"""

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(
    df,
    test_size=0.2,      # 20% goes to test_df
    random_state=42,    # ensures reproducibility
    shuffle=True        # shuffle before splitting
)



"""Plot a bar graph such that you can see the distribution of race in test and train dataset. Are the proportions of races almost same in train and test datasets ? If not what can you do so that the proportions of races in the test and train datasets are close ?


Hint : Think Sorting on the race/ethnicity column and then applying some logic.
"""

# to clarify if the split of races A, B, C, D and E are nearly say 0.2, 0.15, 0.25, 0.17, 0.23
# then I expect an almost similar split in the train_df and test_df

import matplotlib.pyplot as plt

# Count race distribution
train_counts = train_df['race/ethnicity'].value_counts().sort_index()
test_counts  = test_df['race/ethnicity'].value_counts().sort_index()

plt.figure(figsize=(10,5))
x = range(len(train_counts))

plt.bar(x, train_counts, width=0.4, label='Train', align='edge')
plt.bar([i + 0.4 for i in x], test_counts, width=0.4, label='Test', align='edge')

plt.xticks([i + 0.2 for i in x], train_counts.index)
plt.xlabel("Race/Ethnicity")
plt.ylabel("Count")
plt.title("Race Distribution in Train vs Test Dataset")
plt.legend()
plt.show()

"""Even if it the proportions are almost same think of a way to create the train_df and test_df to have similar proportions of races."""

# Sort by race to group all race categories together
sorted_df = df.sort_values("race/ethnicity")

# Compute split index
split_idx = int(len(sorted_df) * 0.8)

# Create train and test ensuring proportions remain close
train_df_balanced = sorted_df.iloc[:split_idx]
test_df_balanced  = sorted_df.iloc[split_idx:]
print("Train balanced proportions:")
print(train_df_balanced['race/ethnicity'].value_counts(normalize=True).sort_index())

print("\nTest balanced proportions:")
print(test_df_balanced['race/ethnicity'].value_counts(normalize=True).sort_index())

"""Usually when we train machine learning models we use batches. Each batch is a subset of train_df of legth batch_size. Create Batches: a list of batch each of size 50 from the train_df"""

batch_size = 50

batches = [
    train_df.iloc[i:i+batch_size]
    for i in range(0, len(train_df), batch_size)
]

len(batches)

"""Print the size of Batches and first few rows of Batches[0]"""

batches[0].head()
